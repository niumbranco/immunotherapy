# Immunotherapy Microbiome Analysis (PRJEB61942) 

This repository contains all scripts, workflows, documentation and ressources related to the analysis of the PRJEB61942 dataset, which investigates the link between the microbiome and response to anti-PD-1 immunotherapy in melanoma patients.

## Requirements: 

### Conda environments
Tree separate conda environments were used to manage dependencies and ensure reproducibility

#### fastp environment:
Used for quality filtering of FASTQ files
<pre>
  conda create -n fastp_env -c bioconda fastp
  conda activate fastp_env
</pre>

#### DADA2 environment: 
Used for sequence processing and ASV inference in R 
<pre>
  conda create -n r_env r-base r-essentials -y
  conda activate r_env
</pre>
And we add the line: 
<pre>
  conda install -c bioconda -c conda-forge bioconductor-dada2 
</pre>
This line allows to install the DADA2 package and all its dependencies directly into the r_env conda environment, ensuring compatibility with the R version and avoiding potential installation issues when using install.packages() inside R.

The `bioconductor-dada2` package contains all functions needed for denoising, merging, chimera removal, and taxonomic assignment in 16S pipelines. 

#### Snakemake environment: 
This environment is used to run the automated workflow 
<pre>
  conda create -n snakemake -c conda-forge -c bioconda snakemake
  conda activate snakemake
  snakemake --version
</pre>
This environment only contains Snakemake.

### Reference databases 
This pipeline uses the SILVA reference database (version 138.1) for taxonomy assignment with DADA2. We must download the following files manally:
- [SILVA v138.1 Train Set](https://zenodo.org/records/4587955#:~:text=silva_nr99_v138.1_wSpecies_train_set.fa.gz)
- [SILVA v138.1 Species Assignment](https://zenodo.org/records/4587955#:~:text=silva_species_assignment_v138.1.fa.gz)
Place both files in a local folder called `database/`. 


## Workflow overview: 

### 1. Data selection
From the ENA metadata, I selected only the **16S rRNA** samples based on the `tax_id=9606` and confirmed the sequencing method 

### 2. Downloading the FASTQ files 
Using a script (`scripts/downlaod_fastq.sh`), I downloaded all paired-end FASTQ files related to 16S sequencing from the ENA FTP links. The dataset is publicly available here: [PRJEB61942 - ENA](https://www.ebi.ac.uk/ena/browser/view/PRJEB61942).

### 3. Files integrity check 
To make sure files were correctly downloaded: 
- I used the **md5 checksums** provided in the metadata
- I wrote a bash script (`scipts/verify_md5.sh`) to compare them with local files
- A log of passed and failed checks was created 
  
### 4. Quality control with fastp 
I ran fastp on all paired-end FASTQ files using default settings (`scripts/run_fastp.sh`)
It cleaned low-quality reads and generated JSON and HTML reports for each sample. 

For fastp, we used the following [this](https://github.com/OpenGene/fastp) tutorial.

### 5. Quality summary and filtering decision
An R script (`scripts/fastp_summary.r`) was created to parse the JSON reports generated by fastp.
It extracts important metrics (number of reads before and after filtering, Q20 and Q30 rates), summarizes them into a tale and produces multiple visualizations. 
**Key observations:**
* Q20 and Q30 scores remain high for all samples
* The main filtering criterion was he number of reads after filtering.

So finally, only samples with less than 30,000 reads after filtering were excluded from downstream analysis.
A detailed visual report explaining this is available: `Notes/fastp_summary_report.pdf`.

### 6. DADA2 processing
The DADA2 algorithm was used to process the 16S rRNA sequencing data. It allows high resolution inference of Amplicon Sequence Variants (ASVs) by correcting sequencing errors without clustering into OTUs.
We followed the official [DADA2 tutorial](https://benjjneb.github.io/dada2/tutorial.html) as a guideline, adapting it to our dataset and processing contraints. 

#### 6.1 Manual version: 
As a first step, DADA2 was run manually to explore and validate each part of the workflow. This was done using a single R script: 
`scripts/dada2.R`

The main steps performed were: 
- **filtering and trimming** with `filterAndTrim()`
- **learning error rates** using `learnErrors()`
- **dereplication** with `derepFastq()`
- **ASV inference** using `dada()`
- **Merging paired-end reads** via `mergePairs()`
- **sequence table construction** with `makeSequenceTable()`
- **Chimera removal** using `removeBimeraDenovo()`
- **Taxonomic assignement** using SILVA reference databases with `assignTaxonomy()` and `addSpecies()`

This setp-by-step approach allowed to verify that each DADA2 step ran correctly before developing a full automated pipeline.

#### 6.2 Automated pipeline: 

To streamline the process and make it reusable on other datasets, the DADA2 analysis was restructured into a **modular and automated pipeline** using multiple R scripts, coordinated through a bash script: 
`scripts/dada2_automated/dada2_auto_pipeline.sh`

## Snakemake workflow: 

The automated workflow (`workflow_smk/`) reproduced the entire analysis from FASTQ to diversity metrics. 

### Run example
<pre>
snakemake --cores 4 --use-conda \
--configfile config/config.yml \
--snakefile amplicon_workflow.smk
</pre>

### Outputs
- **alpha diversity table:** `alpha_div.tsv`
- **beta diversity matrice:** `bray_matrix.tsv`
- **combined diversity plots:**
  - `diversity_combined_plot_all.pdf` → which is created if we run the `diversity_test.R` script manually
  - `diversity_combined_plot.png`→ which is created if we run the entire workflow 


NB: The `PRJEB61942/raw_reads/`, `reads_clean/`, and `reads_taxonomy/` folders are generated by the workflow but not included in the repository due to size.

## File structure: 

### scripts/:
- **General utilities**
  - `download_fastq.sh` → download FATSQ files from ENA
  - `verify_md5.sh` → check the md5 integrity of downloaded files
  - `run_fastp.sh` → quality filtering with fast
  - `fastp_summary.r` → generate plots and summary from fastp JSON reports

- **DADA 2 (manual)**
  - `dada2.R` → full manual DADA2 pipeline (filtering, denoising, mering, chimera removal, taxonomy assignment)
  
- **DADA 2 automated pipeline**
  - `dada2_automated/dada2_auto_pipeline.sh` → main bash script to run the automated pipeline
  - `dada2_automated/filt.R` →  filtering and trimming
  - `dada2_automated/err.R`→ learn error rates
  - `dada2_automated/infer_ASV.R → infer ASVs
  - `dada2_automated/merge_pairs.R` → merge paired-end reads
  - `dada2_automated/seqtab.R` → create sequence table
  - `dada2_automated/taxonomy.R`→ assign taxonomy with SILVA 

- **DADA 2 automated (for Snakemake)**
  - `dada2_automated/for_snakemake/filt_smk.R` → filtering and trimming
  - `dada2_automated/for_snakemake/err_smk.R` → learn error rates 
  - `dada2_automated/for_snakemake/infer_ASV_smk.R` → infer ASVs
  - `dada2_automated/for_snakemake/merge_pairs_smk.R` → merge paired-end reads
  - `dada2_automated/for_snakemake/seqtab_smk.R` → create sequence table
  - `dada2_automated/for_snakemake/taxonomy_smk.R` → assign taxonomy 
  - `dada2_automated/for_snakemake/diversity_smk.R` → diversity analysis (inside workflow) 
  - `dada2_automated/for_snakemake/diversity_test_smk.R` → diversity analysis (manual test, outside workflow) 

### workflow_smk/: 
- `amplicon_workflow_smk` → main Snakemake workflow
- `config/config_yml` → configuration file for workflow parameters
- `rules/diversity.rule → diversity analysis rule
- `rules/dada2.rule → dada2 analysis rule
- `rules/fastp.rule → fastp analysis rule

### metadata/: 
- `filereport_16S.txt` → cleaned metadata with 16S samples
- `fastq_links_16S.txt` → list of FASTQ URLs to download
- `filereport_16S_filtered.tsv` → metadata filtered to keep only good quality samples
- `fastp_summary_table.csv` → table with reads and quality scores

### logs/: 
- `md5_check_ok.txt` → files that passed the md5 check
- `md5_check_fail.txt` → files that failed or were missing
- `log_download.txt` → output log of the download script

### Notes/:
- `fastp_summary_report.pdf` → visual explaination of filtering decisions
- `BIOINFORMATIC_ANALYSES_SUMMARY.pdf` → summary of the plots that I had with some explanations
- `comparison_report.pdf` → fastp comparison between a low and a high quality sample 




