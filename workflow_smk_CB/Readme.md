# ðŸ§¬ 16S Amplicon Sequencing Workflow (Snakemake + Fastp + DADA2)

[![Snakemake](https://img.shields.io/badge/Snakemake-â‰¥7.0-brightgreen.svg)](https://snakemake.github.io)
[![DADA2](https://img.shields.io/badge/DADA2-High%20Resolution%20Denoising-blue.svg)](https://benjjneb.github.io/dada2/index.html)

---

## Overview

This repository contains a workflow implemented in **Snakemake** to run **16S rRNA amplicon analyses**.  
It is designed for reproducible and automated microbiome workflows.  
The pipeline performs:

- Read quality control with **fastp**
- Amplicon sequence variant (ASV) inference and taxonomic assignment with **DADA2**, using the **SILVA database [1]**
- Diversity and community composition visualizations


---  
## Requirements

Before running the workflow, ensure that you have:

1. Raw 16S paired-end FASTQ files.
2. A table listing sample IDs and file names.
3. Snakemake installed and available in your system PATH.

Snakemake is the workflow manager that executes the pipeline and automatically creates all rule-specific environments (for `fastp`, `DADA2`, and R). It must be installed once before running the workflow. The recommended method is to create a dedicated Conda environment for it:

```bash
conda create -n snakemake -c conda-forge -c bioconda snakemake mamba -y
conda activate snakemake
snakemake --version
```
This ensures that Snakemake and its dependencies are correctly configured on your system.
You should see a version number (e.g., 7.32.4) when running the last command, confirming that the installation was successful.

---
## Repository Structure
```
16S_pipeline/
â”œâ”€â”€ Snakefile                 # main Snakemake workflow
â”œâ”€â”€ config.yml                # configuration file (paths, parameters)
â”œâ”€â”€ envs/                     # conda environments per step
â”‚   â”œâ”€â”€ fastp_env.yml
â”‚   â”œâ”€â”€ dada2_env.yml
â”‚   â””â”€â”€ r_env.yml
â”œâ”€â”€ database/                 # taxonomy database
â”‚   â”œâ”€â”€ silva_nr99_v138.1_wSpecies_train_set.fa
â”‚   â”œâ”€â”€ silva_species_assignment_v138.1.fa
â”‚   â”œâ”€â”€ silva_nr99_v138.1_wSpecies_train_set.fa
â”‚   â””â”€â”€ silva_species_assignment_v138.1.fa
â”œâ”€â”€ rules/                    # modular rule files
â”‚   â”œâ”€â”€ fastp.rule
â”‚   â”œâ”€â”€ dada2.rule
â”‚   â””â”€â”€ diversity_analysis.rule
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ for_snakemake/        # R scripts called by Snakemake
â”‚   â”‚   â”œâ”€â”€ err_smk.R
â”‚   â”‚   â”œâ”€â”€ fastp_summary_smk.R
â”‚   â”‚   â”œâ”€â”€ filt_smk.R
â”‚   â”‚   â”œâ”€â”€ infer_ASV_smk.R
â”‚   â”‚   â”œâ”€â”€ plot_taxonomy_composition_smk.R
â”‚   â”‚   â”œâ”€â”€ prepare_tables_smk.R
â”‚   â”‚   â”œâ”€â”€ seqtb_smk.R
â”‚   â”‚   â”œâ”€â”€ remove_chimeras_smk.R
â”‚   â”‚   â”œâ”€â”€ silva_smk.R
â”‚   â”‚   â””â”€â”€ track_reads_smk.R
â”‚   â””â”€â”€ utility/        # scripts to facilitate other tasks
â”‚       â”œâ”€â”€ download_taxonomy_database.sh
â”‚       â””â”€â”€ download_fastq.sh
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fastp/
â”‚   â”‚   â”œâ”€â”€ reads_raw/              # input FASTQs
â”‚   â”‚   â””â”€â”€ reads_clean/         # filtered reads after fastp and quality reports
â”‚   â””â”€â”€ dada2/       # intermediate DADA2 outputs (seqtab, taxa, etc.)
â”œâ”€â”€ results/                  # final outputs
â”‚   â”œâ”€â”€ taxonomy_counts_P.tsv
â”‚   â”œâ”€â”€ taxonomy_counts_G.tsv
â”‚   â””â”€â”€ visuals/
â”‚       â”œâ”€â”€ microbial_composition_per_sample_P.pdf
â”‚       â”œâ”€â”€ microbial_composition_overview_P.pdf
â”‚       â”œâ”€â”€ microbial_composition_per_sample_G.pdf
â”‚       â””â”€â”€ microbial_composition_overview_G.pdf
â””â”€â”€ README.md
```


### 1. Snakefile
The Snakefile is the central workflow definition that controls the execution of all pipeline steps.  
It loads configuration variables, defines sample inputs, and includes all rule modules required for the analysis.

### 2. Configuration file (`config.yml`)

All user-defined parameters and paths used by the workflow are stored in a single configuration file: `config.yml`. It allows the workflow to be reused with minimal changes by simply editing this file

#### `â€ƒâ€¢` General paths

- **input_dir** â†’ Directory containing the raw paired-end FASTQ files.  
- **fastp_dir** â†’ Destination for filtered and cleaned reads generated by `fastp`.  
- **env_dir** â†’ Path to the folder containing the Conda environment `.yml` files.  
- **dada2_dir** â†’ Working directory where DADA2 outputs and intermediate `.rds` files are stored.  
- **sample_table** â†’ Path to the tab-delimited file listing sample identifiers and corresponding FASTQ file names (e.g., `data/samples_ID.tsv`).
- **results** â†’ Directory containing the results. 

#### `â€ƒâ€¢` fastp parameters

Settings used for read quality filtering and trimming that should be adjusted depending on sequencing quality.

```yaml
fastp:
  threads: 4                # Number of CPU threads
  quality_threshold: 20     # Minimum base quality (Phred)
  unqualified_limit: 30     # Maximum % of low-quality bases allowed per read
  length_threshold: 100     # Minimum read length after trimming
  complexity_threshold: 30  # Minimum sequence complexity
```
#### `â€ƒâ€¢` DADA2 parameters
Settings used for ASV inference and taxonomic assignment. 

```yaml
dada2:
  trunc_len_f: 240                  # Forward read truncation length
  trunc_len_r: 200                  # Reverse read truncation length
  silva_train: "database/silva_train_test10k.fa"   # SILVA training set file. UPDATE WITH THE CORRECT FILES USED FOR THE TAXONOMY ASSIGNMENT!
  silva_species: "database/silva_species_test10k.fa" # SILVA species reference UPDATE WITH THE CORRECT FILES USED FOR THE TAXONOMY ASSIGNMENT!
```

### 3. Conda environments
All environments to run the snakemake workflow can be built automatically using the provided `.yml` files within `envs/` folder, or manually as described below.

#### `â€ƒâ€¢` fastp environment (`fastp_env.yml`)

Used for quality filtering of FASTQ files.

```bash
conda create -n fastp_env -c bioconda fastp -y
conda activate fastp_env
fastp --version
```

#### `â€ƒâ€¢` DADA2 environment (`dada2_env.yml`)

Used for sequence processing, ASV inference, and taxonomy assignment in R.  
You can create this environment manually as follows:

```bash
conda create -n dada2_env -c bioconda -c conda-forge -c defaults \
  "r-base=4.3.*" \
  "bioconductor-dada2=1.30.0" \
  "tbb<2021" \
  "r-rcppparallel" \
  "r-tidyverse" -y
```
This installs DADA2 and all dependencies directly into the R environment, ensuring compatibility with the R version and avoiding issues with *install.packages()* inside R.  
To verify installation:
```bash
library(dada2)
packageVersion("dada2")
```

#### `â€ƒâ€¢` R environment (`r_env.yml`)

Used for downstream analyses and visualization of DADA2 results.  
You can create it manually with:

```bash
conda create -n r_env -c conda-forge -c bioconda -c defaults \
  "r-base=4.2" \
  "r-tidyverse" \
  "r-vegan" \
  "r-ggpubr" \
  "r-ape" \
  "r-compositions" \
  "r-dplyr" \
  "r-tidyr" \
  "r-readr" \
  "r-ggplot2" \
  "r-stringr" \
  "r-purrr" \
  "r-data.table" \
  "r-optparse" \
  "r-jsonlite" \
  "r-RColorBrewer" -y
conda activate r_env
```

### 4. Taxonomy database

This workflow uses the **SILVA ribosomal RNA reference database (version 138.1)** for taxonomic assignment with DADA2.  
The SILVA database provides high-quality, curated 16S rRNA gene sequences and taxonomy references for accurate microbial classification.

Due to the large size of the full SILVA datasets, this repository includes only a small subset of the reference files for demonstration and testing purposes.  
A utility script is provided to automatically download and prepare the complete database (or any other database) into the correct directory (`scripts/utility/download_taxonomy_database.sh`).  
Alternatively, the SILVA files can be downloaded manually from the official repository:

- SILVA v138.1 Train Set  
  [https://zenodo.org/records/4587955#:~:text=silva_nr99_v138.1_wSpecies_train_set.fa.gz](https://zenodo.org/records/4587955#:~:text=silva_nr99_v138.1_wSpecies_train_set.fa.gz)

- SILVA v138.1 Species Assignment  
  [https://zenodo.org/records/4587955#:~:text=silva_species_assignment_v138.1.fa.gz](https://zenodo.org/records/4587955#:~:text=silva_species_assignment_v138.1.fa.gz)

After downloading, decompress both files and place them in the local `database/` folder:

```text
database/
â”œâ”€â”€ silva_nr99_v138.1_wSpecies_train_set.fa
â””â”€â”€ silva_species_assignment_v138.1.fa
```
### 5. Rules
The workflow is divided into three main rule files, located in the `rules/` directory:
- `fastp.rule` â†’ read filtering, trimming, and quality control.  
- `dada2.rule` â†’ DADA2 denoising, merging, chimera removal, and taxonomy assignment.  
- `diversity_analysis.rule` â†’ downstream analyses of ASV and taxonomy tables.  

Each parent rule has its own Conda environment and includes smaller rules for independent execution.  

### 6. Scripts

The `scripts/` directory is divided into two subfolders:

#### `â€ƒâ€¢` `for_snakemake/`
- Contains all R scripts used within the Snakemake workflow to run DADA2 and downstream analyses.  
- The workflow follows the [official DADA2 tutorial](https://benjjneb.github.io/dada2/tutorial.html), except for the *Evaluate accuracy* step.  
- For downstream analyses, we chose not to use Phyloseq, as often seen in other workflows. Instead, we convert DADA2 outputs into readable tabular files, facilitating flexible downstream exploration with other tools.

#### `â€ƒâ€¢` `utility/`
Includes helper scripts that can be run directly from the userâ€™s environment.  
These scripts allow users to:  
- Download the raw FASTQ files for the samples `download_fastq.sh`
- Download and prepare the taxonomy database required for the workflow `download_taxonomy_database.sh`


### 7. Data

#### `â€ƒâ€¢` `fastp/`
- `reads_raw/` â†’ paired-end FASTQ files from 16S rRNA amplicon sequencing to be filtered and trimmed with fastp.  
Due to their size, raw reads are not provided in this repository.
However, a utility script is available that can be used to automatically download and prepare the sequencing data for analysis (`scripts/utility/download_fastq.sh`).  
Each sample must include two FASTQ files (forward and reverse reads), typically following the naming convention:
```bash
sampleID_1.fastq.gz
sampleID_2.fastq.gz
```

- `reads_clean/` â†’ after running fastp, we obtain cleaned paired-end reads that were trimmed and filtered out according to the following parameters indicated in `config.yml` (i.e., reads with quality_threshold < 20, unqualified_limit > 30, length_threshold < 100 or complexity_threshold < 30 were removed).


#### `â€ƒâ€¢` `dada2/`
The `dada2/` directory contains all intermediate files generated during the DADA2 steps (e.g., filtering, error learning, merging, chimera removal, and taxonomy assignment).   
These files are mainly stored in `.rds` format and are used internally by the workflow to pass data between rules. They are not required for interpretation of the final results but are kept to ensure reproducibility and traceability of each processing step.

### 8. Results 
The `results/` directory contains only the readable output files, not the intermediate DADA2 objects (`.rds` files). These outputs summarize the main taxonomic and compositional results in tabular and graphical formats.

| File | Description |
|------|--------------|
| `taxonomy_counts_P.tsv` | ASV counts per sample (Phylum level) |
| `taxonomy_counts_G.tsv` | ASV counts per sample (Genus level) |
| `microbial_composition_per_sample_*.pdf` | Stacked barplot showing microbial composition per sample |
| `microbial_composition_overview_*.pdf` | Average microbial composition across all samples |

## Notes
FASTQ files, complete taxonomy database, intermediate .rds, and .snakemake/ folders are excluded from git using .gitignore.




